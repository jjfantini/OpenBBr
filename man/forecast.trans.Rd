% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/functions.R
\name{forecast.trans}
\alias{forecast.trans}
\title{Performs Transformer forecasting}
\usage{
forecast.trans(
  data,
  target_column = "close",
  n_predict = 5,
  train_split = 0.85,
  past_covariates,
  forecast_horizon = 5,
  input_chunk_length = 14,
  output_chunk_length = 5,
  d_model = 64,
  nhead = 4,
  num_encoder_layers = 3,
  num_decoder_layers = 3,
  dim_feedforward = 512,
  activation = "relu",
  dropout = 0,
  batch_size = 32,
  n_epochs = 300,
  learning_rate = 0.001,
  model_save_name = "trans_model",
  force_reset = TRUE,
  save_checkpoints = TRUE,
  metric = "mape"
)
}
\arguments{
\item{target_column}{(character length 1) Target column to forecast. Defaults to "close".}

\item{n_predict}{(integer length 1) Days to predict. Defaults to 5.}

\item{train_split}{(numeric length 1) Train/val split. Defaults to 0.85.}

\item{forecast_horizon}{(integer length 1) Forecast horizon when performing historical forecasting. Defaults to 5.}

\item{input_chunk_length}{(integer length 1) Number of past time steps that are fed to the forecasting module at prediction time. Defaults to 14.}

\item{output_chunk_length}{(integer length 1) The length of the forecast of the model. Defaults to 5.}

\item{d_model}{(integer length 1) The number of expected features in the encoder/decoder inputs. Defaults to 64.}

\item{nhead}{(integer length 1) The number of heads in the multi-head attention mechanism. Defaults to 4.}

\item{num_encoder_layers}{(integer length 1) The number of encoder layers in the encoder. Defaults to 3.}

\item{num_decoder_layers}{(integer length 1) The number of decoder layers in the encoder. Defaults to 3.}

\item{dim_feedforward}{(integer length 1) The dimension of the feedforward network model. Defaults to 512.}

\item{activation}{(character length 1) The activation function of encoder/decoder intermediate layer, ‘relu’ or ‘gelu’. Defaults to 'relu'.}

\item{dropout}{(numeric length 1) Fraction of neurons afected by Dropout. Defaults to 0.0.}

\item{batch_size}{(integer length 1) Number of time series (input and output sequences) used in each training pass. Defaults to 32.}

\item{n_epochs}{(integer length 1) Number of epochs over which to train the model. Defaults to 100.}

\item{learning_rate}{(numeric length 1) Defaults to 1e-3.}

\item{model_save_name}{(character length 1) Name for model. Defaults to "brnn_model".}

\item{force_reset}{(logical length 1) If set to True, any previously-existing model with the same name will be reset (all checkpoints will be
discarded). Defaults to True.}

\item{save_checkpoints}{(logical length 1) Whether or not to automatically save the untrained model and checkpoints from training. Defaults to True.}

\item{metric}{(character length 1) Metric to use for model selection. Defaults to "mape".}
}
\description{
Wrapper for Python function forecast.trans from OpenBB Terminal SDK
}
\examples{
forecast.trans(n_predict=5, target_column='close', train_split=0.85, forecast_horizon=5, input_chunk_length=14, output_chunk_length=5, d_model=64, nhead=4, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=512, activation='relu', dropout=0, batch_size=32, n_epochs=100, learning_rate=0.001, model_save_name='trans_model', force_reset=TRUE, save_checkpoints=TRUE)
}
